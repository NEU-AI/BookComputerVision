{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个agent怎么在一个复杂不确定的环境里，最大化它获得的奖励。在强化学习的结构里，环境和agent不断交互，agent在环境里获取到状态和奖励，agent根据状态输出一个Action(决策)，这个决策放到环境中去，环境根据决策进行处理再输出状态和奖励，所以agent就可以理解为在环境里获取更过的奖励。\n",
    "和监督学习的对比：监督学习一定是给出了正确的标签和一系列没有关联的图片数据是iid 独立同分布（iid，independently identically distribution）的，在不断的学习中去拿预测的结果和正确的标签做比较，不断的完善预测。 监督式学习就好比你在学习的时候，有一个导师在旁边指点，他知道怎么是对的怎么是错的。\n",
    "\n",
    "强化学习和监督学习不同，在很多实际问题中，例如下棋这种游戏，有成千上万种组合方式的情况，不可能有一个导师知道所有可能的结果。在没有任何标签的情况下，通过先尝试做出一些行为得到一个结果，通过这个结果是对还是错的反馈，调整之前的行为，就这样不断的调整，算法能够学习到在什么样的情况下选择什么样的行为可以得到最好的结果。所以强化学习的样本是动作的序列，即是一个序列的数据，数据是有关联的。是通过结果反推这个动作是好还是坏。通过强化学习，一个 agent 可以在探索和开发（exploration and exploitation）之间做权衡，并且选择一个最大的回报。 exploration 会尝试很多不同的事情，看它们是否比以前尝试过的更好。 exploitation 是尝试过去经验中最有效的行为。所以强化学习是一个不断试错的过程。它的反馈信息是延迟的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
