{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习\n",
    "\n",
    "维基百科里对强化学习的定义为：受到行为心理学的启发，强化学习主要关注智能体如何在环境中采取不同的行动，以最大限度地提高累积奖励。\n",
    "\n",
    "强化学习主要由Agent(智能体，主体，玩家等叫法，智能体这种叫法属于直译，玩家这种叫法更贴切一些），Environment(环境)，State(状态)，Action(动作或者行为)，Reward(奖励)组成。如下图显示。\n",
    "\n",
    "![avatar](.\\pic\\qh1.png)\n",
    "\n",
    "Agent玩家是强化学习的主体，我们训练它的行为策略，它在执行了某个动作后会转换到一个新的状态。对于新的状态，环境会给出奖励的信号。随后玩家根据新的状态和奖励，按照一定的策略执行新的动作。Environment环境在不同的情形下会不同，比如AlphaGo环境就是围棋棋盘盘面的情况。对于无人驾驶，环境就是当时路况等情况。Action动作就是主体所要做出的反应或者输出，对于AlphaGo来说就是把棋子下在什么地方，对于无人驾驶就是方向，加速或刹车等指令。Reward(奖励)可以看成是一种反馈，正值就是奖励，负值就是惩罚。在AlphaGo下了一个棋子后，盘面状态发生改变，如果盘面的改变有利就可以得到一个正值的Reward，有利程度越大Reward就会越大，相反如果不利得到负值。\n",
    "\n",
    "![avatar](.\\pic\\qh2.png)\n",
    "\n",
    "Agent玩家通过强化学习不断尝试，总结出在不同的情形下奖励较大的工作方式，也就是说最后它能知道自己在什么状态下，应该采取什么样的动作使得自身获得最大奖励。\n",
    "\n",
    "因此强化学习实际是Agent和Environment进行交互的过程中，学会最佳决策序列，即在连续的时间序列里的最优策略。\n",
    "\n",
    "\n",
    "# 强化学习与监督学习\n",
    "\n",
    "和监督学习的对比：监督学习一定是给出了正确的标签和一系列没有关联的图片数据是iid 独立同分布（iid，independently identically distribution）的，在不断的学习中去拿预测的结果和正确的标签做比较，不断的完善预测。 监督式学习就好比你在学习的时候，有一个导师在旁边指点，他知道怎么是对的怎么是错的。\n",
    "\n",
    "强化学习和监督学习不同，在很多实际问题中，例如下棋这种游戏，有成千上万种组合方式的情况，不可能有一个导师知道所有可能的结果。在没有任何标签的情况下，通过先尝试做出一些行为得到一个结果，通过这个结果是对还是错的反馈，调整之前的行为，就这样不断的调整，算法能够学习到在什么样的情况下选择什么样的行为可以得到最好的结果。所以强化学习的样本是动作的序列，即是一个序列的数据，数据是有关联的。是通过结果反推这个动作是好还是坏。通过强化学习，一个 agent 可以在探索和开发（exploration and exploitation）之间做权衡，并且选择一个最大的回报。 exploration 会尝试很多不同的事情，看它们是否比以前尝试过的更好。 exploitation 是尝试过去经验中最有效的行为。所以强化学习是一个不断试错的过程，进而选择出使得奖励最大化的动作，所以它的反馈信息是延迟的。\n",
    "\n",
    "\n",
    "# 强化学习的特点\n",
    "\n",
    "强化学习有一下五个特点：\n",
    "\n",
    "* 没有监督者，只有奖励：监督学习需要基于大量的标注数据进行训练，在不断的学习中去拿预测的结果和正确的标签做比较，不断的完善预测。 监督式学习就好比你在学习的时候，有一个导师在旁边指点，他知道怎么是对的怎么是错的。而强化学习没有监督者，例如下棋这种游戏，有成千上万种组合方式的情况，不可能有一个导师知道所有可能的结果。只能从环境的反馈中获得奖励。\n",
    "\n",
    "* 反馈延迟：反馈延迟实际上是延迟奖励。环境可能不会在每一步的动作上获得奖励，有时候需要完成一系列的动作，甚至完成整个任务才能获得奖励。\n",
    "\n",
    "* 试错学习： 因为没有监督，所以没有直接的指导信息，玩家需要不断与环境进行交互，通过试错的方式来获得最优策略。\n",
    "\n",
    "* 玩家的动作会影响后续数据：玩家选择不同的动作，会进入不同的状态。下一步所获得的状态发生变化，环境的反馈也会随之变化。\n",
    "\n",
    "* 时间序列：机器学习的其他模型可以接收随机的输入，而强化学习需要输入数据的序列性，下一步额输入依赖与前一步的状态。\n",
    "\n",
    "\n",
    "\n",
    "# 马尔可夫决策过程\n",
    "\n",
    "## 马尔可夫性质\n",
    "\n",
    "在时刻t+1步时，环境的反馈仅仅取决于上一时刻t的状态s和动作a，与时刻t-1以及它之前的任何一部都没有关联性。 即系统的下一个状态只与当前的状态有关，而与之前更早的状态无关。实际环境中所需完成的任务不能完全满足马尔可夫性质，但为了简化问题的求解过程，需要通过约束环境的状态使得问题满足马尔科夫性质。\n",
    "\n",
    "## 马尔可夫决策过程\n",
    "\n",
    "一个马尔可夫决策过程由一个四元组构成，即MDP=(S,A,P_{sa},R)。\n",
    "\n",
    "* S为状态空间集：s<sub>i</sub>表示i时刻的状态，其中S={$s_1,s_2,\\ldots,s_n$}\n",
    "\n",
    "* A为动作空间集：a<sub>i</sub>表示i时刻的动作，其中A={$a_1,a_2,\\ldots,a_n$}\n",
    "\n",
    "* P<sub>sa</sub>为状态转移概率：表示在当前状态s下执行动作a后，转移到另一个状态$s{}'$的概率分布，记作$p(s{}'|s,a)$；如果带有获得的奖励r，则记作$p(s{}',r|s,a)$。\n",
    "\n",
    "* R为奖励函数：在状态s下执行动作a后转移到状态$s{}'$获得的奖励r，中r=R(s,a)。\n",
    "\n",
    "![avatar](.\\pic\\m1.png)\n",
    "\n",
    "如上图所示为马尔可夫决策过程。具体而言，玩家的初始状态为$s_0$，选择动作$a_0$并执行后，按照0.5的概率转移到下一个状态$s_1$($P(s_1|s_0,a_0)=0.5$,获得对应的奖励$r_0=1$；同理选择动作$a_1$执行后，按照概率0.4转移到下一个状态$s_2$，并获得对应的奖励$r_1=2$，按照上述的方式进行直到任务完成，此学习流程即为一个典型的马尔可夫决策过程的强化学习任务的求解过程。\n",
    "\n",
    "## 策略\n",
    "\n",
    "在强化学习中，玩家主要是通过在环境中的不断试错，学习到一个最优的策略$\\pi$。玩家学习到一个策略后，会在后面任何时刻的状态s下都能得到接下来的动作a，经过一段时间的执行后，会获得此策略下的累积奖励的期望，这个期望成为价值v。\n",
    "\n",
    "策略分为确定性策略和随机性策略两种。确定性策略$a=\\pi(s)$就是在状态s下选择动作a而没有概率，动作是确定的。随机性策略$\\pi(s,a)$是在状态s下选择动作a的概率，策略通过概率来表示。对于相同的状态，其输出的状态并不唯一，而是满足一定的概率分布，从而导致即使是处在相同的状态，也可能输出不同的动作。\n",
    "\n",
    "有了策略后，就需要衡量策略的优劣，而这种衡量取决于玩家在长期执行某个策略后得到的累积奖励，而强化学习的目的就是寻找累积奖励最大化额策略。\n",
    "\n",
    "## 奖励\n",
    "\n",
    "为了找到长期累积奖励，不仅需要考虑当前时刻的奖励，还需要考虑未来的奖励。所以总奖励R的计算公式为$R=r_1+r_2+r_3+\\ldots+r_n$。\n",
    "\n",
    "根据总奖励公式可知，长期累积奖励从当前时刻t开始，直到最终状态的奖励$r_n$，得到未来累积奖励$R_t =r_{t+1}+\\ldots+r_n$。一般而言，环境是随机的，下一个状态可能也是随机的。所以无法确定下一次执行相同的动作，以及是否能获得相同的奖励。因此在实际中，会使用衰减因子$\\gamma$来计算折扣未来累积奖励$G_t$来代替未来累积奖励。$G_t=R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\ldots$。\n",
    "\n",
    "其中$\\gamma$介于[0,1]的常数。对于距离当前时刻越远的奖励，其重要性就越低。可以时刻t+1的累积奖励表达公式为$G_t=R_{t+1}+\\gamma( R_{t+2} + \\gamma(R_{t+3}+\\ldots))=R_{t+1}+\\gamma G_{t+1}$。\n",
    "\n",
    "衰减因子$\\gamma$为0就是只考虑当前的即时奖励$r_t$。若想平衡当前时间的奖励和未来的奖励，可以设置较大的值如0.9。如果环境恒定，不需要进行衰减计算，可以设置1。所以强化学习的目标就是玩家选择一个能最大化衰减未来累积奖励的最优策略。\n",
    "\n",
    "## 价值函数\n",
    "\n",
    "当执行到某一步时，如果需要评估当前玩家在该时刻状态的好坏程度，就需要用价值函数来完成。价值函数分为状态价值函数和动作价值函数。状态价值函数描述的是从状态s出发，根据策略$\\pi$ 得到的期望累计回报，在相同的环境下，价值函数会随着策略$\\pi$ 的变化而变化。动作价值函数描述的是从状态s出发，采取策略a之后，根据策略$\\pi$ 得到的期望累计回报。\n",
    "\n",
    "状态价值函数：$ v(s)=\\mathbb{E}[G_t|s_t=s]$\n",
    "动作价值函数：$ q(s,a)=\\mathbb{E}[G_t|s_t=s,a_t=a]$\n",
    "\n",
    "## 最优策略\n",
    "\n",
    "关于马尔可夫的最优策略，有如下定理：\n",
    "1. 对于任何马尔可夫问题，存在一个最优策略，好于（至少相等）任何其他策略\n",
    "2. 所有的最优策略下都有相同的最优状态价值函数\n",
    "3. 所有的最优策略下都具有相同的最优动作价值函数\n",
    "\n",
    "既然有上述定理，那么寻找最优策略就可以通过最大化最优行为价值函数来得到。同时如果我们知道最优行为价值函数，则表明我们找到了最优策略，也就是说寻找最优策略可以通过寻找最优状态值函数得到。\n",
    "\n",
    "在最优值函数中，如果求出了最优状态值函数，其实也就求出了最优行为值函数，反之同理；在最优策略中，如果求出了最优值函数，那么最优策略就求出来了，反之同理；这样就把最优策略、最优状态值函数、最优行为值函数联系起来了。前面假设了这些前提，那么如何求最优状态值函数或者最优行为值函数呢？这就需要使用贝尔曼最优方程了。\n",
    "\n",
    "贝尔曼最优方程一般通过一些迭代方法来解决：价值迭代；策略迭代；Q学习；Sarsa等。只要采用这些方法把最优状态值函数、最优行为值函数或者最优策略求出来了，那么马尔可夫就完全解决了。\n",
    "\n",
    "---\n",
    "贝尔曼方程（可以不用看，了解即可）\n",
    "\n",
    "贝尔曼方程表示当前时刻状态的价值$v(s_t)$和下一时刻状态的价值$v(s{_t+1})$之间的关系。方程表示如下：\n",
    "$ v(s)=\\mathbb{E}[G_t|s_t=s]$\n",
    "\n",
    "&emsp;&emsp; $=\\mathbb{E}[R_{t+1}+\\gamma( R_{t+2} + \\gamma(R_{t+3}+\\ldots)|s_t=s]$\n",
    "\n",
    "&emsp;&emsp; $=\\mathbb{E}[R_{t+1}+\\gamma G_{t+1}|s_t=s]$\n",
    "\n",
    "&emsp;&emsp; $=\\mathbb{E}[R_{t+1}+\\gamma v(S_{t+1})|s_t=s]$\n",
    "\n",
    "通过方程可以看出 $v(s)$ 由两部分组成，一是该状态的即时奖励期望$R_{t+1}$，即时奖励期望等于即时奖励，因为根据即时奖励的定义，它与下一个状态无关；另一个是下一时刻状态的价值期望，可以根据下一时刻状态的概率分布得到其期望。\n",
    "\n",
    "如果用s’表示s状态下一时刻任一可能的状态，那么Bellman方程可以写成：\n",
    "\n",
    "$v(s)=R_s+\\gamma \\displaystyle \\sum_{s' \\in s}{P_{s's}v(s')}$\n",
    "\n",
    "即当前状态s的价值函数，由当前状态获得的奖励$R_s$加上经过状态间转换概率$P_{s's}$乘以下一状态的状态价值函数$v(s')$得到，其中$\\gamma$是未来衰减因子。\n",
    "\n",
    "矩阵形式表示如下：\n",
    "\n",
    "$v=R + \\gamma P v$\n",
    "\n",
    "$  \\left[ \\begin{matrix} v(1)\\\\ \\vdots\\\\ v(n) \\end{matrix} \\right] = \\left[ \\begin{matrix} R(1)\\\\ \\vdots\\\\ R(n) \\end{matrix} \\right] + \\gamma \\left[ \\begin{matrix} P_{11} & \\ldots & P_{1n}\\\\ \\vdots\\\\ P_{11} & \\ldots & P_{nn} \\end{matrix} \\right] \\left[ \\begin{matrix} v(1)\\\\ \\vdots\\\\ v(n) \\end{matrix} \\right]$\n",
    "\n",
    "以下图为例说明如何求解状态的价值。\n",
    "\n",
    "<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"./pic/brm.jpeg\"  width=\"400\" height=\"460\" ></div>\n",
    "<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"./pic/brm2.png\" width=\"400\" height=\"460\" ></div>\n",
    "\n",
    "<div style=\"float:none;clear:both;\">\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "求解每个价值需要针对每个状态列出方程，其中有5个状态列出5个方程即可求解出每个状态的价值,$\\gamma=1$。例如：class1 = -2 + 1.0(0.5*class2 + 0.5*facebook) facebook = -1 + 1.0*(0.9*facebook + 0.1*class1)等进行求解即可\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# QLearning \n",
    "\n",
    "QLearning是强化学习算法中的一种算法，Q即为Q（s,a）就是在某一时刻的s状态下(s∈S)，采取动作a(a∈A)动作能够获得收益的期望，环境会根据agent的动作反馈相应的回报r，所以算法的主要思想就是将State与Action构建成一张Q-table来存储Q值，然后根据Q值来选取能够获得最大收益的动作。\n",
    "\n",
    "Q-Learning算法流程如下：\n",
    "\n",
    "初始化 任意s状态和动作a\n",
    "\n",
    "&emsp;$q(s,a)\\leftarrow 任意值$\n",
    "\n",
    "重复 经验轨迹：\n",
    "\n",
    "&emsp; 初始化状态s\n",
    "\n",
    "&emsp;&emsp;重复每一步的经验轨迹：\n",
    "\n",
    "&emsp;&emsp;&emsp;根据动作状态值q选择动作a\n",
    "\n",
    "&emsp;&emsp;&emsp;执行动作a，获得奖励r和下一刻状态s’\n",
    "\n",
    "&emsp;&emsp;&emsp;$q(s,a) \\leftarrow q(s,a)+\\alpha [r+\\gamma \\displaystyle \\max_{a} q(s',a')-q(s,a) ]$,更新动作值函数\n",
    "\n",
    "&emsp;&emsp;&emsp;$s \\leftarrow s'$,记录状态\n",
    "\n",
    "&emsp;&emsp;直到终止s状态\n",
    "\n",
    "&emsp;输出q(s,a)\n",
    "\n",
    " 根据上面的算法假如要从s1状态通过值较大的a2动作进入s2状态。我们在s1采取了a2, 并到达s2,这时我们需要更新用于决策的Q表,但是接着我们并没有在实际中采取任何行为,而是在想象自己要到s2状态上采取的每一种行为, 哪一个的Q值大,比如说Q(s2, a2)的值比Q(s2, a1) 的大, 所以我们把大的 Q(s2, a2) 乘上一个衰减值 gamma (比如是0.9) 并加上到达s2时所获取的奖励r, 我们将这个作为我现实中 Q(s1, a2) 的值, 但是我们之前是根据 Q 表估计 Q(s1, a2) 的值. 所以有了现实和估计值, 我们就能更新Q(s1, a2) , 根据估计与现实的差距, 将这个差距乘以一个学习效率 alpha 累加上老的 Q(s1, a2) 的值变成新的值. 但时刻记住, 我们虽然用 maxQ(s2) 估算了一下 s2 状态, 但还没有在s2做出任何的行为, s2 的行为决策要等到更新完了以后再重新另外做. 这就是Q-learning 是如何决策和学习优化决策的过程.alpha是学习率, 来决定这次的误差有多少是要被学习的, alpha是一个小于1的数.gamma是对未来reward的衰减值。\n",
    " \n",
    " $Q(s1,a2)现实：r+\\gamma \\displaystyle \\max q(s2)$\n",
    "\n",
    " $Q(s1,a2)估计:Q(s1,a2)$\n",
    "\n",
    " 差距 = 现实-估计\n",
    "\n",
    "$新Q(s1,a2) = 老Q(s1,a2) + \\alpha 差距$\n",
    "\n",
    "下面我们通过一个例子来讲解QLearning的使用。比较经典的例子是走房间参考“http://mnemstudio.org/path-finding-q-learning-tutorial.htm” ，这个例子假设学习率$\\alpha=1$，比较容易理解，有一个房子里有五个房间，房间之间通过门连接，如下图所示，我们把每个房间使用0到4个数字进行编号，最外围当做最大的房间编号为5。\n",
    "![](.\\pic\\modeling_environment_clip_image002a.gif)\n",
    "上图可以简化为一个图来表示，圆圈为房间，如果两个房间有门相连，就用直线进行连接。如下图所示。\n",
    "![](.\\pic\\map1a.gif)\n",
    "对于这个例子，我们的玩家agent位于房子里的任一一个房间里，然后从这个房间出发，让它能走到房子外面去即到达房间5。所以我们需要给每个门即对应的边一个奖励值，直接连接到5号房间的奖励为100，其他门的奖励为0，因为门可进可出，所以是两个方向，所以一个房间上会指定两个箭头，每个箭头带有奖励值。如下图所示。\n",
    "![](.\\pic\\map2a.gif)\n",
    "编号5的房间有一个指向自己的箭头，奖励为100，其他直接指向房间5的边奖励也为100.所以当玩家到达房间5后结束行走，这个为结束的条件。\n",
    "\n",
    "强化学习的两个重要概念就是动作和状态，所以我们将每个房间称为状态，玩家从一个房间走到另一个房间称为动作。我们可以把状态作为行，动作作为列，构建一个矩阵R，矩阵表示奖励，没有相连的房间为-1，相连的为0，连接到房间5的为100。如下图所示。\n",
    "![](.\\pic\\r_matrix1.gif)\n",
    "类似的我们还要构建一个矩阵Q，它用来表示玩家已经从经验中学到知识，即Qlearning中的Q值。强化学习是一个不断试错的过程，所以玩家在没有任何指导的情况下，不断从一个房间走到另一个房间进行探索，指导到达房间5.玩家的每一次探索称为一个回合episode，在每一个回合中，玩家从任意位置到达房间5，当到达房间5这个回合结束，接着进入另一个回合，不断的重复。\n",
    "\n",
    "下面我们来理解一下Q-Learning是如何工作的。首先我们假设$\\gamma$=0.8，初始状态为房间1，并将Q矩阵初始化为一个零矩阵。观察R矩阵第二行，它包含两个非负值。即房间3或房间5。我们随机选取房间5。如下图所示。\n",
    "<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"./pic/q_matrix1.gif\"  width=\"300\" height=\"300\" ></div>\n",
    "<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"./pic/r_matrix1.gif\" width=\"300\" height=\"360\" ></div>\n",
    "\n",
    "<div style=\"float:none;clear:both;\">\n",
    "\n",
    "</div>\n",
    "\n",
    "当我们的玩家位于房间5后，我们观察第6行，它对应三个可能的动作房间1，房间4，房间5.根据公式我们有：\n",
    "\n",
    "Q(s, a) = R(s, a) + Gamma * Max[Q(s', a')]\n",
    "\n",
    "Q(1, 5) = R(1, 5) + 0.8 * Max[Q(5, 1), Q(5, 4), Q(5, 5)] = 100 + 0.8 * 0 = 100\n",
    "\n",
    "现在房间5变成了当前状态，因为房间5为目标，所以这一次回合结束，玩家的Q矩阵进行更新如下图所示。\n",
    "![](.\\pic\\q_matrix2.gif)\n",
    "接下来进行下一次的回合迭代，还是随机选取一个初始状态，比如我们选择房间3作为初始状态。观察R矩阵的第4行，它对应三个可能的动作：走到房间1，房间2或房间4。随机我们走到房间1。因此观察R矩阵的第二行，它对应有二个可能的动作：走到房间3或房间5。根据公式我们有：\n",
    "\n",
    "Q(3, 1) = R(3, 1) + 0.8 * Max[Q(1, 3), Q(1, 5)] = 0 + 0.8 * Max(0, 100) = 80\n",
    "\n",
    "其中Q(1, 5)用到了前一回合刷新的值100。此时矩阵Q变成了如下。\n",
    "![](.\\pic\\q_matrix3.gif)\n",
    "现在房间1变成当前状态，因为它不是目标房间，所以继续探索，对于房间1对应二个可能的动作：走到房间3或房间5。假如我们随机选择了房间5。对于房间5，它对应三个可能的动作房间1，房间4，房间5。根据公式\n",
    "\n",
    "Q(1, 5) = R(1, 5) + 0.8 * Max[Q(5, 1), Q(5, 4), Q(5, 5)] = 100 + 0.8 * 0 = 100\n",
    "现在房间5变成了当前状态，因为房间5为目标，所以这一次回合结束。我们不断重复这个过程，经过多轮的回合，矩阵Q会变成\n",
    "\n",
    "[[  0.    0.    0.    0.   80.    0. ]\n",
    " [  0.    0.    0.   64.    0.  100. ]\n",
    " [  0.    0.    0.   64.    0.    0. ]\n",
    " [  0.   80.   51.2   0.   80.    0. ]\n",
    " [ 64.    0.    0.   64.    0.  100. ]\n",
    " [  0.    0.    0.    0.    0.    0. ]]\n",
    "\n",
    "一旦Q矩阵完成构建，玩家agent便学会了转移至目标的最佳路径。即选择每个状态最大价值的路线行走。如下图。\n",
    "![](.\\pic\\map5.gif)\n",
    "例如，从房间2为初始状态，利用Q表可得，\n",
    "\n",
    "* 从房间2，选择最大Q值指向房间3\n",
    "\n",
    "* 从房间3，选择最大Q值指向房间1或者4，如果选择1\n",
    "\n",
    "* 从房间1，选择最大Q值指向房间5\n",
    "\n",
    "因此最佳路径就为2-3-1-5。\n",
    "\n",
    "代码实现如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Q = np.zeros([6, 6])#定义一个Q表用来保存期望值\n",
    "R = np.array([[-1, -1, -1, -1, 0, -1],\n",
    "              [-1, -1, -1, 0, -1, 100],\n",
    "              [-1, -1, -1, 0, -1, -1],\n",
    "              [-1, 0, 0, -1, 0, -1],\n",
    "              [0, -1, -1, 0, -1, 100],\n",
    "              [-1, 0, -1, -1, 0, 100]])\n",
    "\n",
    "num_episodes = 100 #回合数\n",
    "gamma = 0.8 #衰减因子\n",
    "epsilon = 0.9#贪婪度\n",
    "alpha = 1#学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练\n",
    "for episode in range(num_episodes):\n",
    "    current = np.random.randint(0, 6)#5个房间里随机选择一个房间作为初始房间\n",
    "    while current != 5:\n",
    "        possible_actions = []\n",
    "        for action in range(6):#循环获取在这个房间里可以走哪几个房间\n",
    "            # print(\"action: %d\" % action)\n",
    "            if R[current, action] >= 0:#不能走的房间值是-1，所以大于-1的房间是可以走的。\n",
    "                possible_actions.append(action)#保存房间号\n",
    "        rand = np.random.rand()#取随机值用来做贪婪判断，10%的情况选择随机，90%的情况选择值最大的值的房间走。\n",
    "        if rand > epsilon:\n",
    "            choose = possible_actions[np.random.randint(0, len(possible_actions))]\n",
    "        else:\n",
    "            choose = np.argmax(R[current])#返回最大值的索引\n",
    "\n",
    "        q_predict = Q[current, choose]#保存估算值\n",
    "\n",
    "        Q[current, choose] += alpha*(R[current, choose] + gamma * np.max(Q[choose, :]) - q_predict)#使用公式计算实际值\n",
    "\n",
    "        #更新当前位置\n",
    "        current = choose\n",
    "    print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试\n",
    "num_episodes = 10#走10个回合\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    current = np.random.randint(0, 6)\n",
    "    print(\"start from %d\" % current)\n",
    "\n",
    "    while current != 5:\n",
    "        choose = np.argmax(Q[current, :])\n",
    "        print(\"go to %d\" % choose)\n",
    "\n",
    "        current = choose\n",
    "    print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再一个寻宝的例子，环境是一个一维世界, 在世界的右边有宝藏, 探索者只要得到宝藏尝到了甜头, 然后以后就记住了得到宝藏的方法, 这就是他用强化学习所学习到的行为。\n",
    "\n",
    "#T 就是宝藏的位置, o 是探索者的位置\n",
    "\n",
    "-o---T\n",
    "\n",
    "探索者在一定状态的行为都会用一个值 Q(s, a)记录, 也就是说行为a在s状态的值是Q(s,a). s在这个的探索者游戏中, 就是o所在的地点. 而每一个地点探索者都能做出两个行为 left/right, 这就是探索者的所有可行的动作a。\n",
    "\n",
    "如果在某个地点 s1, 探索者计算了他能有的两个行为, a1为向左边移动，a2为向右边移动,计算的结果是 Q(s1, a1) > Q(s1, a2), 那么探索者就会选择左边这个行动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o----T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "---o-T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "----oT\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "---o-T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "----oT\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "---o-T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "----oT\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "---o-T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "---o-T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "---o-T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "o----T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "o----T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "o----T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "o----T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "o----T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "o----T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "o----T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "---o-T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "----oT\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "Episode 1: total_steps = 38\n",
      "                                \n",
      "o----T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "o----T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "o----T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "o----T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "---o-T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "o----T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "--o--T\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "---o-T\n",
      "   left  right\n",
      "0   0.0  0.000\n",
      "1   0.0  0.000\n",
      "2   0.0  0.000\n",
      "3   0.0  0.009\n",
      "4   0.0  0.100\n",
      "5   0.0  0.000\n",
      "----oT\n",
      "   left  right\n",
      "0   0.0  0.000\n",
      "1   0.0  0.000\n",
      "2   0.0  0.000\n",
      "3   0.0  0.009\n",
      "4   0.0  0.190\n",
      "5   0.0  0.000\n",
      "Episode 2: total_steps = 22\n",
      "                                \n",
      "o----T\n",
      "   left  right\n",
      "0   0.0  0.000\n",
      "1   0.0  0.000\n",
      "2   0.0  0.000\n",
      "3   0.0  0.009\n",
      "4   0.0  0.190\n",
      "5   0.0  0.000\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0  0.000\n",
      "1   0.0  0.000\n",
      "2   0.0  0.000\n",
      "3   0.0  0.009\n",
      "4   0.0  0.190\n",
      "5   0.0  0.000\n",
      "o----T\n",
      "   left  right\n",
      "0   0.0  0.000\n",
      "1   0.0  0.000\n",
      "2   0.0  0.000\n",
      "3   0.0  0.009\n",
      "4   0.0  0.190\n",
      "5   0.0  0.000\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0  0.000\n",
      "1   0.0  0.000\n",
      "2   0.0  0.000\n",
      "3   0.0  0.009\n",
      "4   0.0  0.190\n",
      "5   0.0  0.000\n",
      "o----T\n",
      "   left  right\n",
      "0   0.0  0.000\n",
      "1   0.0  0.000\n",
      "2   0.0  0.000\n",
      "3   0.0  0.009\n",
      "4   0.0  0.190\n",
      "5   0.0  0.000\n",
      "-o---T\n",
      "   left  right\n",
      "0   0.0  0.000\n",
      "1   0.0  0.000\n",
      "2   0.0  0.000\n",
      "3   0.0  0.009\n",
      "4   0.0  0.190\n",
      "5   0.0  0.000\n",
      "--o--T\n",
      "   left    right\n",
      "0   0.0  0.00000\n",
      "1   0.0  0.00000\n",
      "2   0.0  0.00081\n",
      "3   0.0  0.00900\n",
      "4   0.0  0.19000\n",
      "5   0.0  0.00000\n",
      "---o-T\n",
      "   left    right\n",
      "0   0.0  0.00000\n",
      "1   0.0  0.00000\n",
      "2   0.0  0.00081\n",
      "3   0.0  0.02520\n",
      "4   0.0  0.19000\n",
      "5   0.0  0.00000\n",
      "----oT\n",
      "   left    right\n",
      "0   0.0  0.00000\n",
      "1   0.0  0.00000\n",
      "2   0.0  0.00081\n",
      "3   0.0  0.02520\n",
      "4   0.0  0.27100\n",
      "5   0.0  0.00000\n",
      "Episode 3: total_steps = 9\n",
      "                                \n",
      "o----T\n",
      "   left    right\n",
      "0   0.0  0.00000\n",
      "1   0.0  0.00000\n",
      "2   0.0  0.00081\n",
      "3   0.0  0.02520\n",
      "4   0.0  0.27100\n",
      "5   0.0  0.00000\n",
      "-o---T\n",
      "   left     right\n",
      "0   0.0  0.000000\n",
      "1   0.0  0.000073\n",
      "2   0.0  0.000810\n",
      "3   0.0  0.025200\n",
      "4   0.0  0.271000\n",
      "5   0.0  0.000000\n",
      "--o--T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   left     right\n",
      "0   0.0  0.000000\n",
      "1   0.0  0.000073\n",
      "2   0.0  0.002997\n",
      "3   0.0  0.025200\n",
      "4   0.0  0.271000\n",
      "5   0.0  0.000000\n",
      "---o-T\n",
      "   left     right\n",
      "0   0.0  0.000000\n",
      "1   0.0  0.000073\n",
      "2   0.0  0.002997\n",
      "3   0.0  0.047070\n",
      "4   0.0  0.271000\n",
      "5   0.0  0.000000\n",
      "----oT\n",
      "   left     right\n",
      "0   0.0  0.000000\n",
      "1   0.0  0.000073\n",
      "2   0.0  0.002997\n",
      "3   0.0  0.047070\n",
      "4   0.0  0.343900\n",
      "5   0.0  0.000000\n",
      "Episode 4: total_steps = 5\n",
      "                                \n",
      "o----T\n",
      "   left     right\n",
      "0   0.0  0.000007\n",
      "1   0.0  0.000073\n",
      "2   0.0  0.002997\n",
      "3   0.0  0.047070\n",
      "4   0.0  0.343900\n",
      "5   0.0  0.000000\n",
      "-o---T\n",
      "   left     right\n",
      "0   0.0  0.000007\n",
      "1   0.0  0.000335\n",
      "2   0.0  0.002997\n",
      "3   0.0  0.047070\n",
      "4   0.0  0.343900\n",
      "5   0.0  0.000000\n",
      "--o--T\n",
      "      left     right\n",
      "0  0.00000  0.000007\n",
      "1  0.00000  0.000335\n",
      "2  0.00003  0.002997\n",
      "3  0.00000  0.047070\n",
      "4  0.00000  0.343900\n",
      "5  0.00000  0.000000\n",
      "-o---T\n",
      "      left     right\n",
      "0  0.00000  0.000007\n",
      "1  0.00000  0.000572\n",
      "2  0.00003  0.002997\n",
      "3  0.00000  0.047070\n",
      "4  0.00000  0.343900\n",
      "5  0.00000  0.000000\n",
      "--o--T\n",
      "      left     right\n",
      "0  0.00000  0.000007\n",
      "1  0.00000  0.000572\n",
      "2  0.00003  0.006934\n",
      "3  0.00000  0.047070\n",
      "4  0.00000  0.343900\n",
      "5  0.00000  0.000000\n",
      "---o-T\n",
      "      left     right\n",
      "0  0.00000  0.000007\n",
      "1  0.00000  0.000572\n",
      "2  0.00003  0.006934\n",
      "3  0.00000  0.073314\n",
      "4  0.00000  0.343900\n",
      "5  0.00000  0.000000\n",
      "----oT\n",
      "      left     right\n",
      "0  0.00000  0.000007\n",
      "1  0.00000  0.000572\n",
      "2  0.00003  0.006934\n",
      "3  0.00000  0.073314\n",
      "4  0.00000  0.409510\n",
      "5  0.00000  0.000000\n",
      "Episode 5: total_steps = 7\n",
      "                                \n",
      "o----T\n",
      "      left     right\n",
      "0  0.00000  0.000057\n",
      "1  0.00000  0.000572\n",
      "2  0.00003  0.006934\n",
      "3  0.00000  0.073314\n",
      "4  0.00000  0.409510\n",
      "5  0.00000  0.000000\n",
      "-o---T\n",
      "      left     right\n",
      "0  0.00000  0.000057\n",
      "1  0.00000  0.001138\n",
      "2  0.00003  0.006934\n",
      "3  0.00000  0.073314\n",
      "4  0.00000  0.409510\n",
      "5  0.00000  0.000000\n",
      "--o--T\n",
      "      left     right\n",
      "0  0.00000  0.000057\n",
      "1  0.00000  0.001138\n",
      "2  0.00003  0.012839\n",
      "3  0.00000  0.073314\n",
      "4  0.00000  0.409510\n",
      "5  0.00000  0.000000\n",
      "---o-T\n",
      "      left     right\n",
      "0  0.00000  0.000057\n",
      "1  0.00000  0.001138\n",
      "2  0.00003  0.012839\n",
      "3  0.00000  0.102839\n",
      "4  0.00000  0.409510\n",
      "5  0.00000  0.000000\n",
      "----oT\n",
      "      left     right\n",
      "0  0.00000  0.000057\n",
      "1  0.00000  0.001138\n",
      "2  0.00003  0.012839\n",
      "3  0.00000  0.102839\n",
      "4  0.00000  0.468559\n",
      "5  0.00000  0.000000\n",
      "Episode 6: total_steps = 5\n",
      "                                \n",
      "o----T\n",
      "      left     right\n",
      "0  0.00000  0.000154\n",
      "1  0.00000  0.001138\n",
      "2  0.00003  0.012839\n",
      "3  0.00000  0.102839\n",
      "4  0.00000  0.468559\n",
      "5  0.00000  0.000000\n",
      "-o---T\n",
      "      left     right\n",
      "0  0.00000  0.000154\n",
      "1  0.00000  0.002180\n",
      "2  0.00003  0.012839\n",
      "3  0.00000  0.102839\n",
      "4  0.00000  0.468559\n",
      "5  0.00000  0.000000\n",
      "--o--T\n",
      "      left     right\n",
      "0  0.00000  0.000154\n",
      "1  0.00000  0.002180\n",
      "2  0.00003  0.020810\n",
      "3  0.00000  0.102839\n",
      "4  0.00000  0.468559\n",
      "5  0.00000  0.000000\n",
      "---o-T\n",
      "      left     right\n",
      "0  0.00000  0.000154\n",
      "1  0.00000  0.002180\n",
      "2  0.00003  0.020810\n",
      "3  0.00000  0.134725\n",
      "4  0.00000  0.468559\n",
      "5  0.00000  0.000000\n",
      "----oT\n",
      "      left     right\n",
      "0  0.00000  0.000154\n",
      "1  0.00000  0.002180\n",
      "2  0.00003  0.020810\n",
      "3  0.00000  0.134725\n",
      "4  0.00000  0.521703\n",
      "5  0.00000  0.000000\n",
      "Episode 7: total_steps = 5\n",
      "                                \n",
      "o----T\n",
      "      left     right\n",
      "0  0.00000  0.000335\n",
      "1  0.00000  0.002180\n",
      "2  0.00003  0.020810\n",
      "3  0.00000  0.134725\n",
      "4  0.00000  0.521703\n",
      "5  0.00000  0.000000\n",
      "-o---T\n",
      "      left     right\n",
      "0  0.00000  0.000335\n",
      "1  0.00000  0.003835\n",
      "2  0.00003  0.020810\n",
      "3  0.00000  0.134725\n",
      "4  0.00000  0.521703\n",
      "5  0.00000  0.000000\n",
      "--o--T\n",
      "      left     right\n",
      "0  0.00000  0.000335\n",
      "1  0.00000  0.003835\n",
      "2  0.00003  0.030854\n",
      "3  0.00000  0.134725\n",
      "4  0.00000  0.521703\n",
      "5  0.00000  0.000000\n",
      "---o-T\n",
      "      left     right\n",
      "0  0.00000  0.000335\n",
      "1  0.00000  0.003835\n",
      "2  0.00003  0.030854\n",
      "3  0.00000  0.168206\n",
      "4  0.00000  0.521703\n",
      "5  0.00000  0.000000\n",
      "----oT\n",
      "      left     right\n",
      "0  0.00000  0.000335\n",
      "1  0.00000  0.003835\n",
      "2  0.00003  0.030854\n",
      "3  0.00000  0.168206\n",
      "4  0.00000  0.569533\n",
      "5  0.00000  0.000000\n",
      "Episode 8: total_steps = 5\n",
      "                                \n",
      "o----T\n",
      "      left     right\n",
      "0  0.00000  0.000647\n",
      "1  0.00000  0.003835\n",
      "2  0.00003  0.030854\n",
      "3  0.00000  0.168206\n",
      "4  0.00000  0.569533\n",
      "5  0.00000  0.000000\n",
      "-o---T\n",
      "      left     right\n",
      "0  0.00000  0.000647\n",
      "1  0.00000  0.006228\n",
      "2  0.00003  0.030854\n",
      "3  0.00000  0.168206\n",
      "4  0.00000  0.569533\n",
      "5  0.00000  0.000000\n",
      "--o--T\n",
      "      left     right\n",
      "0  0.00000  0.000647\n",
      "1  0.00000  0.006228\n",
      "2  0.00003  0.042907\n",
      "3  0.00000  0.168206\n",
      "4  0.00000  0.569533\n",
      "5  0.00000  0.000000\n",
      "---o-T\n",
      "      left     right\n",
      "0  0.00000  0.000647\n",
      "1  0.00000  0.006228\n",
      "2  0.00003  0.042907\n",
      "3  0.00000  0.202643\n",
      "4  0.00000  0.569533\n",
      "5  0.00000  0.000000\n",
      "----oT\n",
      "      left     right\n",
      "0  0.00000  0.000647\n",
      "1  0.00000  0.006228\n",
      "2  0.00003  0.042907\n",
      "3  0.00000  0.202643\n",
      "4  0.00000  0.612580\n",
      "5  0.00000  0.000000\n",
      "Episode 9: total_steps = 5\n",
      "                                \n",
      "o----T\n",
      "      left     right\n",
      "0  0.00000  0.001142\n",
      "1  0.00000  0.006228\n",
      "2  0.00003  0.042907\n",
      "3  0.00000  0.202643\n",
      "4  0.00000  0.612580\n",
      "5  0.00000  0.000000\n",
      "-o---T\n",
      "      left     right\n",
      "0  0.00000  0.001142\n",
      "1  0.00000  0.009467\n",
      "2  0.00003  0.042907\n",
      "3  0.00000  0.202643\n",
      "4  0.00000  0.612580\n",
      "5  0.00000  0.000000\n",
      "--o--T\n",
      "      left     right\n",
      "0  0.00000  0.001142\n",
      "1  0.00000  0.009467\n",
      "2  0.00003  0.056855\n",
      "3  0.00000  0.202643\n",
      "4  0.00000  0.612580\n",
      "5  0.00000  0.000000\n",
      "---o-T\n",
      "      left     right\n",
      "0  0.00000  0.001142\n",
      "1  0.00000  0.009467\n",
      "2  0.00003  0.056855\n",
      "3  0.00000  0.237511\n",
      "4  0.00000  0.612580\n",
      "5  0.00000  0.000000\n",
      "----oT\n",
      "      left     right\n",
      "0  0.00000  0.001142\n",
      "1  0.00000  0.009467\n",
      "2  0.00003  0.056855\n",
      "3  0.00000  0.237511\n",
      "4  0.00000  0.651322\n",
      "5  0.00000  0.000000\n",
      "Episode 10: total_steps = 5\n",
      "                                \n",
      "o----T\n",
      "      left     right\n",
      "0  0.00000  0.001880\n",
      "1  0.00000  0.009467\n",
      "2  0.00003  0.056855\n",
      "3  0.00000  0.237511\n",
      "4  0.00000  0.651322\n",
      "5  0.00000  0.000000\n",
      "-o---T\n",
      "      left     right\n",
      "0  0.00000  0.001880\n",
      "1  0.00000  0.013637\n",
      "2  0.00003  0.056855\n",
      "3  0.00000  0.237511\n",
      "4  0.00000  0.651322\n",
      "5  0.00000  0.000000\n",
      "--o--T\n",
      "      left     right\n",
      "0  0.00000  0.001880\n",
      "1  0.00000  0.013637\n",
      "2  0.00003  0.072545\n",
      "3  0.00000  0.237511\n",
      "4  0.00000  0.651322\n",
      "5  0.00000  0.000000\n",
      "---o-T\n",
      "      left     right\n",
      "0  0.00000  0.001880\n",
      "1  0.00000  0.013637\n",
      "2  0.00003  0.072545\n",
      "3  0.00000  0.272379\n",
      "4  0.00000  0.651322\n",
      "5  0.00000  0.000000\n",
      "----oT\n",
      "      left     right\n",
      "0  0.00000  0.001880\n",
      "1  0.00000  0.013637\n",
      "2  0.00003  0.072545\n",
      "3  0.00000  0.272379\n",
      "4  0.00000  0.686189\n",
      "5  0.00000  0.000000\n",
      "Episode 11: total_steps = 5\n",
      "                                \n",
      "o----T\n",
      "      left     right\n",
      "0  0.00000  0.002920\n",
      "1  0.00000  0.013637\n",
      "2  0.00003  0.072545\n",
      "3  0.00000  0.272379\n",
      "4  0.00000  0.686189\n",
      "5  0.00000  0.000000\n",
      "-o---T\n",
      "      left     right\n",
      "0  0.00000  0.002920\n",
      "1  0.00000  0.018803\n",
      "2  0.00003  0.072545\n",
      "3  0.00000  0.272379\n",
      "4  0.00000  0.686189\n",
      "5  0.00000  0.000000\n",
      "--o--T\n",
      "      left     right\n",
      "0  0.00000  0.002920\n",
      "1  0.00000  0.018803\n",
      "2  0.00003  0.089805\n",
      "3  0.00000  0.272379\n",
      "4  0.00000  0.686189\n",
      "5  0.00000  0.000000\n",
      "---o-T\n",
      "      left     right\n",
      "0  0.00000  0.002920\n",
      "1  0.00000  0.018803\n",
      "2  0.00003  0.089805\n",
      "3  0.00000  0.306898\n",
      "4  0.00000  0.686189\n",
      "5  0.00000  0.000000\n",
      "----oT\n",
      "       left     right\n",
      "0  0.000000  0.002920\n",
      "1  0.000000  0.018803\n",
      "2  0.000030  0.089805\n",
      "3  0.000000  0.306898\n",
      "4  0.027621  0.686189\n",
      "5  0.000000  0.000000\n",
      "---o-T\n",
      "       left     right\n",
      "0  0.000000  0.002920\n",
      "1  0.000000  0.018803\n",
      "2  0.000030  0.089805\n",
      "3  0.000000  0.337965\n",
      "4  0.027621  0.686189\n",
      "5  0.000000  0.000000\n",
      "----oT\n",
      "       left     right\n",
      "0  0.000000  0.002920\n",
      "1  0.000000  0.018803\n",
      "2  0.000030  0.089805\n",
      "3  0.000000  0.337965\n",
      "4  0.027621  0.717570\n",
      "5  0.000000  0.000000\n",
      "Episode 12: total_steps = 7\n",
      "                                \n",
      "o----T\n",
      "       left     right\n",
      "0  0.000000  0.004320\n",
      "1  0.000000  0.018803\n",
      "2  0.000030  0.089805\n",
      "3  0.000000  0.337965\n",
      "4  0.027621  0.717570\n",
      "5  0.000000  0.000000\n",
      "-o---T\n",
      "       left     right\n",
      "0  0.000000  0.004320\n",
      "1  0.000000  0.025005\n",
      "2  0.000030  0.089805\n",
      "3  0.000000  0.337965\n",
      "4  0.027621  0.717570\n",
      "5  0.000000  0.000000\n",
      "--o--T\n",
      "       left     right\n",
      "0  0.000000  0.004320\n",
      "1  0.000000  0.025005\n",
      "2  0.000030  0.111241\n",
      "3  0.000000  0.337965\n",
      "4  0.027621  0.717570\n",
      "5  0.000000  0.000000\n",
      "---o-T\n",
      "       left     right\n",
      "0  0.000000  0.004320\n",
      "1  0.000000  0.025005\n",
      "2  0.000030  0.111241\n",
      "3  0.000000  0.368750\n",
      "4  0.027621  0.717570\n",
      "5  0.000000  0.000000\n",
      "----oT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       left     right\n",
      "0  0.000000  0.004320\n",
      "1  0.000000  0.025005\n",
      "2  0.000030  0.111241\n",
      "3  0.000000  0.368750\n",
      "4  0.027621  0.745813\n",
      "5  0.000000  0.000000\n",
      "Episode 13: total_steps = 5\n",
      "                                \n",
      "\n",
      "Q-table:\n",
      "\n",
      "       left     right\n",
      "0  0.000000  0.004320\n",
      "1  0.000000  0.025005\n",
      "2  0.000030  0.111241\n",
      "3  0.000000  0.368750\n",
      "4  0.027621  0.745813\n",
      "5  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "np.random.seed(2)  # 随机\n",
    "\n",
    "\n",
    "N_STATES = 6   #1维世界的宽度\n",
    "ACTIONS = ['left', 'right']     # 探索者的可用动作\n",
    "EPSILON = 0.9   #  贪婪度 greedy\n",
    "ALPHA = 0.1     # 学习率\n",
    "GAMMA = 0.9    # 衰减因子\n",
    "MAX_EPISODES = 13   # 最大回合数\n",
    "FRESH_TIME = 0.3    # 移动间隔时间\n",
    "\n",
    "#创建Q表 index 是所有对应的 state (探索者位置), columns 是对应的 action (探索者行为，左，右).\n",
    "def build_q_table(n_states, actions):\n",
    "    table = pd.DataFrame(\n",
    "        np.zeros((n_states, len(actions))),     # q_table 全 0 初始\n",
    "        columns=actions,    # columns 对应的是行为名称\n",
    "    )\n",
    "    #print(table)\n",
    "    return table\n",
    "\n",
    "#定义探索者是如何选择行动. 需要使用到epsilon greedy 的概念. 因为在初始阶段, \n",
    "#随机的探索环境, 往往比固定的行为模式要好, 所以这也是累积经验的阶段, 我们希望探索者不会那么贪婪(greedy). \n",
    "#所以 EPSILON 就是用来控制贪婪程度的值. EPSILON 可以随着探索时间不断提升(越来越贪婪), \n",
    "#不过在这个例子中, 我们就固定成 EPSILON = 0.9, 90% 的时间是选择最优策略, 10% 的时间来探索.\n",
    "def choose_action(state, q_table):\n",
    "    # 选出这个 state 的所有 action 值\n",
    "    state_actions = q_table.iloc[state, :]\n",
    "    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):  # 非贪婪 or 或者这个 state 还没有探索过\n",
    "        action_name = np.random.choice(ACTIONS)\n",
    "    else:# 贪婪模式\n",
    "        action_name = state_actions.idxmax()\n",
    "    return action_name\n",
    "\n",
    "#做出行为后, 环境也要给我们的行为一个反馈, 反馈出下个 state (S_) 和 在上个 state (S) 做出 action (A) 所得到的 reward (R). \n",
    "#这里定义的规则就是, 只有当 o 移动到了 T, 探索者才会得到唯一的一个奖励, 奖励值 R=1, 其他情况都没有奖励.\n",
    "def get_env_feedback(S, A):\n",
    "    if A == 'right':    #向右边移动\n",
    "        if S == N_STATES - 2:   #当到达最后一个state时再向右边移动就到达了终点\n",
    "            S_ = 'terminal'\n",
    "            R = 1\n",
    "        else:#因为终点在右边所以在没到达终点时只要向右边移动就距离终点进一步，但是没有奖励\n",
    "            S_ = S + 1\n",
    "            R = 0\n",
    "    else:   # 向左边移动\n",
    "        R = 0\n",
    "        if S == 0:\n",
    "            S_ = S  \n",
    "        else:#因为终点在右边所以向左边移动就距离终点远一步，但是没有奖励\n",
    "            S_ = S - 1\n",
    "    return S_, R\n",
    "\n",
    "#更新环境\n",
    "def update_env(S, episode, step_counter):\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' 画移动的环境\n",
    "    if S == 'terminal':#如果等于terminal说明到达了终点。显示最终结果\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('{}'.format(interaction))\n",
    "        time.sleep(2)\n",
    "        print('                                ')        \n",
    "    else:#否则打印出移动到了哪个位置\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('{}'.format(interaction))\n",
    "        time.sleep(FRESH_TIME)\n",
    "\n",
    "#QLearning主要算法实现\n",
    "def ql():\n",
    "    q_table = build_q_table(N_STATES, ACTIONS)#初始Q表\n",
    "    for episode in range(MAX_EPISODES):#控制回合数\n",
    "        step_counter = 0\n",
    "        S = 0# 回合初始位置\n",
    "        is_terminated = False# 是否回合结束\n",
    "        update_env(S, episode, step_counter)# 环境更新\n",
    "        while not is_terminated:\n",
    "\n",
    "            A = choose_action(S, q_table)# 选行为\n",
    "            S_, R = get_env_feedback(S, A)   # 实施行为并得到环境的反馈\n",
    "            q_predict = q_table.loc[S, A]# 估算的(状态-行为)值\n",
    "            if S_ != 'terminal':\n",
    "                q_target = R + GAMMA * q_table.iloc[S_, :].max()  #  实际的(状态-行为)值 (回合没结束)\n",
    "            else:\n",
    "                q_target = R     #  实际的(状态-行为)值 (回合结束)\n",
    "                is_terminated = True    # 回合结束\n",
    "\n",
    "            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # 更新Q表\n",
    "            print(q_table)\n",
    "            S = S_   # 探索者移动到下一个 state\n",
    "\n",
    "            update_env(S, episode, step_counter+1) # 环境更新\n",
    "            step_counter += 1\n",
    "    return q_table\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q_table = ql()\n",
    "    print('\\r\\nQ-table:\\n')\n",
    "    print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
